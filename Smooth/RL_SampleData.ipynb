{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% import packages and load environment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gym\n",
    "from torch.distributions import Normal\n",
    "import scipy.signal\n",
    "from torch.optim import SGD\n",
    "\n",
    "env = gym.make('Pendulum-v1')\n",
    "\n",
    "import os\n",
    "\n",
    "folder_name = 'Data/Relu'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# %% define the policy NN and some functions\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) * 2\n",
    "\n",
    "        return x\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x)) * 2\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_policy(state, policy):\n",
    "    # function that uses NN to calculate mean of normal distribution, returns the Normal objective\n",
    "\n",
    "    state = torch.as_tensor(np.array(state), dtype=torch.float)\n",
    "    mean = policy(state)\n",
    "    return Normal(mean, 1)\n",
    "\n",
    "\n",
    "def get_action(state, policy):\n",
    "    # sample the Normal distribution and return the result as action\n",
    "\n",
    "    action = get_policy(state, policy).sample()\n",
    "    return action\n",
    "\n",
    "\n",
    "def discount_cumsum(x: np.ndarray, gamma: float) -> torch.tensor:\n",
    "    # copy from Ray v1.9.2 source code https://docs.ray.io/en/releases-1.9.2/_modules/ray/rllib/evaluation/postprocessing.html?highlight=discount_cumsum#\n",
    "    \"\"\"Calculates the discounted cumulative sum over a reward sequence `x`.\n",
    "\n",
    "    y[t] - discount*y[t+1] = x[t]\n",
    "    reversed(y)[t] - discount*reversed(y)[t-1] = reversed(x)[t]\n",
    "\n",
    "    Args:\n",
    "        gamma: The discount factor gamma.\n",
    "\n",
    "    Returns:\n",
    "        The sequence containing the discounted cumulative sums\n",
    "        for each individual reward in `x` till the end of the trajectory.\n",
    "\n",
    "    Examples:\n",
    "        >>> x = np.array([0.0, 1.0, 2.0, 3.0])\n",
    "        >>> gamma = 0.9\n",
    "        >>> discount_cumsum(x, gamma)\n",
    "        ... array([0.0 + 0.9*1.0 + 0.9^2*2.0 + 0.9^3*3.0,\n",
    "        ...        1.0 + 0.9*2.0 + 0.9^2*3.0,\n",
    "        ...        2.0 + 0.9*3.0,\n",
    "        ...        3.0])\n",
    "    \"\"\"\n",
    "    result = scipy.signal.lfilter([1], [1, float(-gamma)], x[::-1], axis=0)[::-1]\n",
    "    return torch.as_tensor(result.copy())\n",
    "\n",
    "\n",
    "def compute_objective(obs_traj, action_traj, reward_traj, policy, discount=0.99):\n",
    "    H = len(reward_traj)\n",
    "    Q_func = discount_cumsum(torch.Tensor.numpy(reward_traj), discount)\n",
    "    weight = discount ** torch.arange(0, H)\n",
    "    log_prob = get_policy(obs_traj, policy).log_prob(action_traj).reshape(-1)\n",
    "\n",
    "    obj_func = Q_func * log_prob * weight\n",
    "    return obj_func.sum()  # .mean()  #\n",
    "\n",
    "\n",
    "def get_samples(policy, maximum_length=100):\n",
    "    # get a single batch trajectory\n",
    "    obs_traj = []\n",
    "    action_traj = []\n",
    "    reward_traj = []\n",
    "\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset(seed=6)\n",
    "\n",
    "    for _ in range(maximum_length):\n",
    "        # take one step\n",
    "        action = get_action(obs, policy)  # stochastic action\n",
    "        # state = torch.as_tensor(np.array(obs), dtype=torch.float)\n",
    "        # action = policy(state).detach() # determinastic action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # save the result\n",
    "        obs_traj.append(obs.copy())\n",
    "        action_traj.append(action)\n",
    "        reward_traj.append(reward)\n",
    "\n",
    "        # update obs\n",
    "        obs = next_state\n",
    "\n",
    "    eps = [obs_traj, action_traj, reward_traj]\n",
    "    return eps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_grad(model):\n",
    "    # get gradient of a NN model\n",
    "    temp_grad = []\n",
    "    for param in model.parameters():\n",
    "        temp_grad.append(param.grad.data.reshape(-1))\n",
    "    return torch.cat([g for g in temp_grad])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% save batch samples to local files\n",
    "sample_batch = 5000\n",
    "\n",
    "torch.manual_seed(6)\n",
    "policy = GaussianPolicy()\n",
    "\n",
    "for i in range(sample_batch):\n",
    "    file_name = 'Data/Relu/Pendulum_data_maximum300_' + str(int(i)) + '.pt'\n",
    "    eps = get_samples(policy, maximum_length=300)\n",
    "    torch.save(eps, file_name)\n",
    "    print('finish file No.%d\\r' % i, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% save gradient\n",
    "discount = 0.97\n",
    "sample_batch = 5000\n",
    "\n",
    "torch.manual_seed(6)\n",
    "policy = GaussianPolicy()\n",
    "\n",
    "# get length of parmeter\n",
    "model_parameters = filter(lambda p: p.requires_grad, policy.parameters())\n",
    "params_num = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "eta = np.linspace(2, 300, 20, dtype=int)\n",
    "sample_variance = np.zeros([len(eta), params_num])\n",
    "sample_mean = np.zeros([len(eta), params_num])\n",
    "\n",
    "optimizer = SGD(policy.parameters(), lr=0.00001)\n",
    "\n",
    "# %% Calculate gradient for each sample\n",
    "for i in range(len(eta)):\n",
    "    H = eta[i]  # maximum length\n",
    "    grad_sample = np.zeros([sample_batch, params_num])\n",
    "\n",
    "    for j in range(sample_batch):\n",
    "        file_name = 'Data/Relu/Pendulum_data_maximum300_' + str(int(j)) + '.pt'\n",
    "        eps = torch.load(file_name)\n",
    "\n",
    "        obs_traj = eps[0]\n",
    "        action_traj = torch.as_tensor(eps[1]).reshape(-1, 1)\n",
    "        reward_traj = torch.as_tensor(eps[2])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = - compute_objective(obs_traj[:H], action_traj[:H],\n",
    "                                   reward_traj[:H], policy, discount)\n",
    "        loss.backward()\n",
    "        grad = get_grad(policy)\n",
    "        grad_sample[j, :] = grad.numpy()\n",
    "    file_name = 'Data/Relu/Pendulum_maximum300_gamma97_grad_H_' + \\\n",
    "        str(int(H)) + '.pt'\n",
    "    torch.save(grad_sample, file_name)\n",
    "    print('finish eta = %d\\r' % H, end=\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
